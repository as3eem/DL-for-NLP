{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocess.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAL1swo63dh6"
      },
      "source": [
        "Topics to Cover in today's workshop:\r\n",
        "\r\n",
        "1. Sentiment Classification\r\n",
        "2. Fake News DEtection\r\n",
        "    a. MLP <br>\r\n",
        "    b. Feed Forward NN <br>\r\n",
        "    c. LSTM <br>\r\n",
        "    d. CNN <br>\r\n",
        "    e. BERT <br>\r\n",
        "    f. BERT + Fine Tuning <br>\r\n",
        "3. Word Embeddings\r\n",
        "    a. Word2Vec <br>\r\n",
        "    b. Glove<br>\r\n",
        "    c. BERT Embeddings<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1favl97e-urt"
      },
      "source": [
        "%matplotlib inline\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "\r\n",
        "import sqlite3\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import nltk\r\n",
        "import string\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import roc_curve, auc\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "\r\n",
        "import re\r\n",
        "import string\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.stem import PorterStemmer\r\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\r\n",
        "\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from gensim.models import KeyedVectors\r\n",
        "import pickle\r\n",
        "\r\n",
        "from tqdm import tqdm\r\n",
        "import os"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYx2CW1Nmyv3"
      },
      "source": [
        "Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IMuBf1SC_AQF",
        "outputId": "636d935d-c0cc-4227-fe9e-90599dc765c8"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/IIITD Data/NLP/workshop/data.tsv', sep='\\t')\r\n",
        "df.head()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The gags , and the script , are a mixed bag .</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is made fresh by an intelligent screenplay and...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>would n't matter so much that this arrogant Ri...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>to ask people to sit still for two hours and c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The story gives ample opportunity for large-sc...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Phrase  Sentiment\n",
              "0      The gags , and the script , are a mixed bag .          0\n",
              "1  is made fresh by an intelligent screenplay and...          1\n",
              "2  would n't matter so much that this arrogant Ri...          0\n",
              "3  to ask people to sit still for two hours and c...          0\n",
              "4  The story gives ample opportunity for large-sc...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJa4Ev2QBQ2c"
      },
      "source": [
        "Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__GAHQgbBk0m"
      },
      "source": [
        "def decontracted(phrase):\r\n",
        "    # specific\r\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\r\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\r\n",
        "\r\n",
        "    # general\r\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\r\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\r\n",
        "    return phrase"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "4GVRMQB8_MDM",
        "outputId": "40552844-8f58-4f5e-a27f-60ed14022c71"
      },
      "source": [
        "# Combining all the above stundents \r\n",
        "from tqdm import tqdm\r\n",
        "from bs4 import BeautifulSoup\r\n",
        "\r\n",
        "preprocessed_reviews = []\r\n",
        "\r\n",
        "for sentance in tqdm(df['Phrase'].values):\r\n",
        "    sentance = re.sub(r\"http\\S+\", \"\", sentance)\r\n",
        "    sentance = BeautifulSoup(sentance, 'lxml').get_text()\r\n",
        "    sentance = decontracted(sentance)\r\n",
        "    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\r\n",
        "    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\r\n",
        "    sentance = ' '.join(e.lower() for e in sentance.split())\r\n",
        "    #uncomment below if you want to remove stopwords too\r\n",
        "    # sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stopwords)\r\n",
        "    preprocessed_reviews.append(sentance.strip())\r\n",
        "\r\n",
        "# new column feature for proecssed reviews\r\n",
        "df['reviews'] = preprocessed_reviews\r\n",
        "df.head()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 12000/12000 [00:03<00:00, 3822.36it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phrase</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The gags , and the script , are a mixed bag .</td>\n",
              "      <td>0</td>\n",
              "      <td>the gags and the script are a mixed bag</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is made fresh by an intelligent screenplay and...</td>\n",
              "      <td>1</td>\n",
              "      <td>is made fresh by an intelligent screenplay and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>would n't matter so much that this arrogant Ri...</td>\n",
              "      <td>0</td>\n",
              "      <td>would not matter so much that this arrogant ri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>to ask people to sit still for two hours and c...</td>\n",
              "      <td>0</td>\n",
              "      <td>to ask people to sit still for two hours and c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The story gives ample opportunity for large-sc...</td>\n",
              "      <td>1</td>\n",
              "      <td>the story gives ample opportunity for large sc...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Phrase  ...                                            reviews\n",
              "0      The gags , and the script , are a mixed bag .  ...            the gags and the script are a mixed bag\n",
              "1  is made fresh by an intelligent screenplay and...  ...  is made fresh by an intelligent screenplay and...\n",
              "2  would n't matter so much that this arrogant Ri...  ...  would not matter so much that this arrogant ri...\n",
              "3  to ask people to sit still for two hours and c...  ...  to ask people to sit still for two hours and c...\n",
              "4  The story gives ample opportunity for large-sc...  ...  the story gives ample opportunity for large sc...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EM-_4jxBTkJ"
      },
      "source": [
        ""
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OHJrNGSCURJ",
        "outputId": "661523de-d8ac-4feb-ed41-959a31dfb85f"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# Data Splitting\r\n",
        "# Train Data :  80%\r\n",
        "# Test Data  :  20%\r\n",
        "\r\n",
        "x = df['reviews']\r\n",
        "y = np.array(df['Sentiment'])\r\n",
        "\r\n",
        "# data split  \r\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=4)\r\n",
        "print(f\"Train Data Size: {x_train.shape[0]}\")\r\n",
        "print(f\"Test Data Size: {x_test.shape[0]}\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Data Size: 9600\n",
            "Test Data Size: 2400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZw6Vj-IEI8N"
      },
      "source": [
        "# Word Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1vxj4H6EOO9"
      },
      "source": [
        "1. Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w27O-1PTDTtf"
      },
      "source": [
        "# specify names for word2vec data\r\n",
        "X_train_w2v, X_test_w2v = list(x_train), list(x_test)\r\n",
        "y_train_w2v, y_test_w2v = y_train, y_test\r\n",
        "\r\n",
        "# create \r\n",
        "i=0\r\n",
        "list_of_sentance_test=[]\r\n",
        "for sentance in X_test_w2v:\r\n",
        "    list_of_sentance_test.append(sentance.split())\r\n",
        "\r\n",
        "i=0\r\n",
        "list_of_sentance=[]\r\n",
        "for sentance in X_train_w2v:\r\n",
        "    list_of_sentance.append(sentance.split())"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQmMtg8AE7lh",
        "outputId": "6981e3ad-5c49-42e8-decf-da2aaabd5cd5"
      },
      "source": [
        "# Using Google News Word2Vectors\r\n",
        "\r\n",
        "# min_count = 5 considers only words that occured atleast 5 times\r\n",
        "w2v_model=Word2Vec(list_of_sentance,min_count=5,size=50, workers=4)\r\n",
        "print(w2v_model.wv.most_similar('great'))\r\n",
        "print('='*50)\r\n",
        "print(w2v_model.wv.most_similar('worst'))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('star', 0.9996079802513123), ('dull', 0.9996078610420227), ('day', 0.9995817542076111), ('compelling', 0.9995776414871216), ('itself', 0.9995694160461426), ('girl', 0.9995529651641846), ('comes', 0.9995518922805786), ('fascinating', 0.9995502233505249), ('work', 0.9995487928390503), ('takes', 0.9995459318161011)]\n",
            "==================================================\n",
            "[('summer', 0.9992620348930359), ('films', 0.9992106556892395), ('rare', 0.9992033839225769), ('work', 0.9991836547851562), ('man', 0.9991766810417175), ('stories', 0.9991741180419922), ('great', 0.9991329312324524), ('picture', 0.9991177916526794), ('version', 0.9991052150726318), ('making', 0.9990689754486084)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUUDEL7XDwpn",
        "outputId": "3a11dba2-8181-4615-d8b6-d3f9f06f8d5c"
      },
      "source": [
        "w2v_words = list(w2v_model.wv.vocab)\r\n",
        "print(\"number of words that occured minimum 5 times \",len(w2v_words))\r\n",
        "print(\"sample words \", w2v_words[0:50])"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of words that occured minimum 5 times  4185\n",
            "sample words  ['the', 'film', 'does', 'not', 'have', 'enough', 'innovation', 'or', 'to', 'attract', 'teenagers', 'and', 'it', 'lacks', 'novel', 'charm', 'that', 'made', 'spy', 'kids', 'a', 'surprising', 'winner', 'with', 'both', 'adults', 'younger', 'audiences', 'maybe', 'how', 'will', 'you', 'feel', 'after', 'an', 'rip', 'off', 'of', 'rock', 'action', 'slo', 'mo', 'gun', 'firing', 'random', 'glass', 'being', 'dreary', 'mid', 'hoffman']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RprsQUoeF0dt",
        "outputId": "16883804-0d2b-4df2-b532-cf257fd1a259"
      },
      "source": [
        "# TRAINING W2V DATA VECTORS\r\n",
        "\r\n",
        "# average Word2Vec\r\n",
        "# compute average word2vec for each review.\r\n",
        "sent_vectors_train = []; # the avg-w2v for each sentence/review is stored in this list\r\n",
        "for sent in tqdm(list_of_sentance): # for each review/sentence\r\n",
        "    sent_vec = np.zeros(50) \r\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\r\n",
        "    for word in sent: # for each word in a review/sentence\r\n",
        "        if word in w2v_words:\r\n",
        "            vec = w2v_model.wv[word]\r\n",
        "            sent_vec += vec\r\n",
        "            cnt_words += 1\r\n",
        "    if cnt_words != 0:\r\n",
        "        sent_vec /= cnt_words\r\n",
        "    sent_vectors_train.append(sent_vec)\r\n",
        "\r\n",
        "print(len(sent_vectors_train))\r\n",
        "print(len(sent_vectors_train[0]))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 9600/9600 [00:03<00:00, 2618.64it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "9600\n",
            "50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYc-xE7wGCAD",
        "outputId": "d2fe4c55-5083-4e60-ea1f-95f6fbc9aacb"
      },
      "source": [
        "# TEST W2V DATA VECTORS\r\n",
        "\r\n",
        "sent_vectors_test = []; # the avg-w2v for each sentence/review is stored in this list\r\n",
        "for sent in tqdm(list_of_sentance_test): # for each review/sentence\r\n",
        "    sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\r\n",
        "    cnt_words =0; # num of words with a valid vector in the sentence/review\r\n",
        "    for word in sent: # for each word in a review/sentence\r\n",
        "        if word in w2v_words:\r\n",
        "            vec = w2v_model.wv[word]\r\n",
        "            sent_vec += vec\r\n",
        "            cnt_words += 1\r\n",
        "    if cnt_words != 0:\r\n",
        "        sent_vec /= cnt_words\r\n",
        "    sent_vectors_test.append(sent_vec)\r\n",
        "    \r\n",
        "print(len(sent_vectors_test))\r\n",
        "print(len(sent_vectors_test[0]))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2400/2400 [00:00<00:00, 2547.39it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2400\n",
            "50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVFwwcyKGHRG"
      },
      "source": [
        "X_train_avgw2v, X_test_avgw2v = sent_vectors_train, sent_vectors_test"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j9LLBKjrHlo"
      },
      "source": [
        "# MODELS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0z3bOX2RIns"
      },
      "source": [
        "MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mid8MP5hwpuO"
      },
      "source": [
        "#relevant libraries\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "import torch"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9vlPbnQwUfA"
      },
      "source": [
        "#data into pytorch tensor\r\n",
        "x_train_w2v = torch.tensor(X_train_avgw2v, dtype=torch.float32)\r\n",
        "x_test_w2v = torch.tensor(X_test_avgw2v, dtype=torch.float32)\r\n",
        "\r\n",
        "y_train_w2v = torch.from_numpy(y_train.astype(np.float32)) \r\n",
        "y_train_w2v = y_train_w2v.view(y_train_w2v.shape[0], 1)\r\n",
        "y_test_w2v = torch.from_numpy(y_test.astype(np.float32)) \r\n",
        "y_test_w2v = y_test_w2v.view(y_test_w2v.shape[0], 1)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gecEkqSMRIJM",
        "outputId": "b9a9de59-83a4-4a99-aa7e-3ed8fd2fa9e2"
      },
      "source": [
        "# do mlp here\r\n",
        "clf = MLPClassifier(random_state=1, max_iter=300, activation='tanh', learning_rate='adaptive').fit(x_train_w2v, y_train_w2v)\r\n",
        "y_pred = clf.predict(x_test_w2v)\r\n",
        "print(\"Accuracy on Test Data\", 100*accuracy_score(y_test_w2v, y_pred))"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on Test Data 57.49999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXi7ovZNS2jc"
      },
      "source": [
        "Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZijD_3NS2LL"
      },
      "source": [
        "# relevant libraries\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-TpRaj4kt62"
      },
      "source": [
        "#data into pytorch tensor\r\n",
        "x_train_w2v = torch.tensor(X_train_avgw2v, dtype=torch.float32)\r\n",
        "x_test_w2v = torch.tensor(X_test_avgw2v, dtype=torch.float32)\r\n",
        "\r\n",
        "y_train_w2v = torch.from_numpy(y_train.astype(np.float32)) \r\n",
        "y_train_w2v = y_train_w2v.view(y_train_w2v.shape[0], 1)\r\n",
        "y_test_w2v = torch.from_numpy(y_test.astype(np.float32)) \r\n",
        "y_test_w2v = y_test_w2v.view(y_test_w2v.shape[0], 1)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OriTixTTC0x"
      },
      "source": [
        " class NeturalNet(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\r\n",
        "        super(NeturalNet, self).__init__()\r\n",
        "        \r\n",
        "        self.l1 = nn.Linear(input_size, hidden_size)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.l2 = nn.Linear(hidden_size, 1)\r\n",
        "        self.actv = nn.Sigmoid()\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        out = self.l1(x)\r\n",
        "        out = self.relu(out)\r\n",
        "        out = self.l2(out)\r\n",
        "        out = self.actv(out)\r\n",
        "        return out # see why we didn't put any softmax layer at the last?"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jet5EA0MUHB5",
        "outputId": "3eab146a-c10b-4108-ac26-55212dfe8b46"
      },
      "source": [
        "# parameters\r\n",
        "input_size = sent_vectors_train[0].shape[0]\r\n",
        "hidden_size = 30\r\n",
        "num_classes = len(np.unique(y_train_w2v))\r\n",
        "\r\n",
        "# call our Neural Nework model here\r\n",
        "model = NeturalNet(input_size, hidden_size, num_classes)\r\n",
        "\r\n",
        "# loss and optimzer\r\n",
        "# loss_def = nn.CrossEntropyLoss()\r\n",
        "loss_def = nn.BCELoss()\r\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\r\n",
        "\r\n",
        "# training loop\r\n",
        "epochs = 2000\r\n",
        "for epoch in range(epochs):\r\n",
        "\r\n",
        "    # forward\r\n",
        "    outputs = model(x_train_w2v)\r\n",
        "    loss = loss_def(outputs, y_train_w2v)\r\n",
        "  \r\n",
        "    # backward\r\n",
        "    optimiser.zero_grad()\r\n",
        "    loss.backward() # dl/dw\r\n",
        "    optimiser.step()\r\n",
        "\r\n",
        "    # print some records\r\n",
        "    if epoch%100 == 0:\r\n",
        "        print(f\"Completed for Epoch={epoch+100} , current loss= {loss.item()}\")"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed for Epoch=100 , current loss= 0.6962115168571472\n",
            "Completed for Epoch=200 , current loss= 0.672905683517456\n",
            "Completed for Epoch=300 , current loss= 0.6682748198509216\n",
            "Completed for Epoch=400 , current loss= 0.6632984280586243\n",
            "Completed for Epoch=500 , current loss= 0.6587013006210327\n",
            "Completed for Epoch=600 , current loss= 0.656630277633667\n",
            "Completed for Epoch=700 , current loss= 0.6530822515487671\n",
            "Completed for Epoch=800 , current loss= 0.6512198448181152\n",
            "Completed for Epoch=900 , current loss= 0.6499900221824646\n",
            "Completed for Epoch=1000 , current loss= 0.6484162211418152\n",
            "Completed for Epoch=1100 , current loss= 0.6547624468803406\n",
            "Completed for Epoch=1200 , current loss= 0.6485295295715332\n",
            "Completed for Epoch=1300 , current loss= 0.6454678177833557\n",
            "Completed for Epoch=1400 , current loss= 0.6535665988922119\n",
            "Completed for Epoch=1500 , current loss= 0.6439165472984314\n",
            "Completed for Epoch=1600 , current loss= 0.6429005861282349\n",
            "Completed for Epoch=1700 , current loss= 0.6437638401985168\n",
            "Completed for Epoch=1800 , current loss= 0.6412592530250549\n",
            "Completed for Epoch=1900 , current loss= 0.642840564250946\n",
            "Completed for Epoch=2000 , current loss= 0.6422010660171509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSpK932movop",
        "outputId": "ca803daa-eeff-4e2b-d099-20caf3d682b3"
      },
      "source": [
        "# for testing\r\n",
        "with torch.no_grad():\r\n",
        "    y_predicted_sig = model(x_test_w2v)\r\n",
        "    y_pred = y_predicted_sig.round() # 0.9 -> 1\r\n",
        "    acc = accuracy_score(y_test_w2v, y_pred)\r\n",
        "print(\"Accuracy of Feed Forward Network\", acc)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Feed Forward Network 0.6004166666666667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}